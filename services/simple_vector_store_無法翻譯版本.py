import json
import pickle
import numpy as np
from typing import List, Dict, Tuple
import logging
from pathlib import Path
import time
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import re

logger = logging.getLogger(__name__)

class SimpleRecipeVectorStore:
    def __init__(self):
        """ä½¿ç”¨ TF-IDF çš„ç°¡å–®å‘é‡åŒ–æ–¹æ¡ˆï¼Œæ”¯æ´ä¸­æ–‡æ–·è©"""
        self.vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),
            stop_words=None,
            tokenizer=self.chinese_tokenizer
        )
        self.tfidf_matrix = None
        self.recipes_metadata = []
        self.is_built = False

    def chinese_tokenizer(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è©å™¨"""
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        tokens = jieba.lcut(text)
        tokens = [token.strip() for token in tokens if len(token) > 1 and not token.isdigit()]
        return tokens

    def build_index_from_chunks(self, processed_data_dir: str = 'data/processed'):
        """å¾è™•ç†å¾Œçš„è³‡æ–™å¡Šå»ºç«‹ TF-IDF ç´¢å¼•"""
        logger.info("é–‹å§‹å»ºç«‹ TF-IDF ç´¢å¼•...")

        index_file = f"{processed_data_dir}/index.json"
        if not Path(index_file).exists():
            raise FileNotFoundError("æ‰¾ä¸åˆ°è™•ç†å¾Œçš„è³‡æ–™ç´¢å¼•æª”æ¡ˆ")

        with open(index_file, 'r', encoding='utf-8') as f:
            index_data = json.load(f)

        chunk_files = index_data['chunk_files']
        all_texts = []

        for chunk_file in chunk_files:
            logger.info(f"è¼‰å…¥ {chunk_file}")
            with open(chunk_file, 'r', encoding='utf-8') as f:
                recipes_chunk = json.load(f)

            for recipe in recipes_chunk:
                all_texts.append(recipe['full_text'])
                self.recipes_metadata.append(recipe)

        logger.info(f"é–‹å§‹å»ºç«‹ TF-IDF çŸ©é™£ï¼Œå…± {len(all_texts):,} ç­†æ–‡å­—...")
        self.tfidf_matrix = self.vectorizer.fit_transform(all_texts)
        self.is_built = True
        logger.info(f"TF-IDF ç´¢å¼•å»ºç«‹å®Œæˆï¼çŸ©é™£å¤§å°: {self.tfidf_matrix.shape}")
        self.save_index()

    def save_index(self, index_dir: str = 'data/embeddings'):
        """å„²å­˜ç´¢å¼•"""
        Path(index_dir).mkdir(parents=True, exist_ok=True)

        with open(f"{index_dir}/tfidf_vectorizer.pkl", 'wb') as f:
            pickle.dump(self.vectorizer, f)

        with open(f"{index_dir}/tfidf_matrix.pkl", 'wb') as f:
            pickle.dump(self.tfidf_matrix, f)

        with open(f"{index_dir}/recipes_metadata.pkl", 'wb') as f:
            pickle.dump(self.recipes_metadata, f)

        logger.info(f"ç´¢å¼•å·²å„²å­˜è‡³ {index_dir}")

    def load_index(self, index_dir: str = 'data/embeddings'):
        """è¼‰å…¥ç´¢å¼•"""
        try:
            with open(f"{index_dir}/tfidf_vectorizer.pkl", 'rb') as f:
                self.vectorizer = pickle.load(f)

            with open(f"{index_dir}/tfidf_matrix.pkl", 'rb') as f:
                self.tfidf_matrix = pickle.load(f)

            with open(f"{index_dir}/recipes_metadata.pkl", 'rb') as f:
                self.recipes_metadata = pickle.load(f)

            self.is_built = True
            logger.info(f"ç´¢å¼•è¼‰å…¥æˆåŠŸï¼çŸ©é™£å¤§å°: {self.tfidf_matrix.shape}")
            return True

        except Exception as e:
            logger.error(f"è¼‰å…¥ç´¢å¼•å¤±æ•—: {e}")
            return False

    def search(self, query: str, k: int = 10, min_similarity: float = 0.1) -> List[Tuple[Dict, float]]:
        """æœå°‹ç›¸ä¼¼é£Ÿè­œ"""
        if not self.is_built:
            raise ValueError("ç´¢å¼•å°šæœªå»ºç«‹")

        start_time = time.time()
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
        top_indices = similarities.argsort()[-k:][::-1]

        results = []
        for idx in top_indices:
            sim_score = float(similarities[idx])
            if sim_score >= min_similarity:
                recipe = self.recipes_metadata[idx]
                results.append((recipe, sim_score))

        search_time = time.time() - start_time
        logger.info(f"ğŸ” æŸ¥è©¢ã€Œ{query}ã€å®Œæˆï¼Œå…±å–å¾— {len(results)} ç­†çµæœï¼Œè€—æ™‚ {search_time:.3f} ç§’")

        if results:
            for i, (r, score) in enumerate(results[:3]):
                logger.info(f"  {i+1}. {r['title']}ï¼ˆscore: {score:.4f}ï¼‰")
        else:
            logger.warning("âš ï¸ æŸ¥ç„¡ç›¸ä¼¼çµæœ")

        return results
